{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhXrma34SNcB"
      },
      "source": [
        "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
        "\n",
        "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P59NYU98GCb9",
        "outputId": "bd187dfb-6023-4f0e-8ba1-c83ae5f925ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==0.4.1 (from versions: 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for torch==0.4.1\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 16.0 MB 5.4 MB/s \n",
            "\u001b[?25h  Building wheel for bokeh (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "panel 0.12.1 requires bokeh<2.4.0,>=2.3.0, but you have bokeh 0.13.0 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 5.4 MB 4.8 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.20.2 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.20.2 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip3 -qq install torch==0.4.1\n",
        "!pip3 -qq install bokeh==0.13.0\n",
        "!pip3 -qq install gensim==3.6.0\n",
        "!pip3 -qq install nltk\n",
        "!pip3 -qq install scikit-learn==0.20.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8sVtGHmA9aBM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6CNKM3b4hT1"
      },
      "source": [
        "# Рекуррентные нейронные сети (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_XkoGNQUeGm"
      },
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFEtWrS_4rUs"
      },
      "source": [
        "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
        "\n",
        "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
        "\n",
        "Мы порешаем сейчас POS Tagging для английского.\n",
        "\n",
        "Будем работать с таким набором тегов:\n",
        "- ADJ - adjective (new, good, high, ...)\n",
        "- ADP - adposition (on, of, at, ...)\n",
        "- ADV - adverb (really, already, still, ...)\n",
        "- CONJ - conjunction (and, or, but, ...)\n",
        "- DET - determiner, article (the, a, some, ...)\n",
        "- NOUN - noun (year, home, costs, ...)\n",
        "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
        "- PRT - particle (at, on, out, ...)\n",
        "- PRON - pronoun (he, their, her, ...)\n",
        "- VERB - verb (is, say, told, ...)\n",
        "- . - punctuation marks (. , ;)\n",
        "- X - other (ersatz, esprit, dunno, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPIkKdFlHB-X"
      },
      "source": [
        "Скачаем данные:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiA2dGmgF1rW",
        "outputId": "7e9389de-fd05-4fb3-ba92-cee8ee484c6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dtype=np.int):\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, positive=False):\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:1097: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:1344: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:1480: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  precompute=False, eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/randomized_l1.py:320: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, random_state=None,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/randomized_l1.py:580: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=4 * np.finfo(np.float).eps, n_jobs=None,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d93g_swyJA_V"
      },
      "source": [
        "Пример размеченного предложения:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QstS4NO0L97c",
        "outputId": "ad2a93aa-7c74-450b-891b-6768428e9c99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The            \tDET\n",
            "Fulton         \tNOUN\n",
            "County         \tNOUN\n",
            "Grand          \tADJ\n",
            "Jury           \tNOUN\n",
            "said           \tVERB\n",
            "Friday         \tNOUN\n",
            "an             \tDET\n",
            "investigation  \tNOUN\n",
            "of             \tADP\n",
            "Atlanta's      \tNOUN\n",
            "recent         \tADJ\n",
            "primary        \tNOUN\n",
            "election       \tNOUN\n",
            "produced       \tVERB\n",
            "``             \t.\n",
            "no             \tDET\n",
            "evidence       \tNOUN\n",
            "''             \t.\n",
            "that           \tADP\n",
            "any            \tDET\n",
            "irregularities \tNOUN\n",
            "took           \tVERB\n",
            "place          \tNOUN\n",
            ".              \t.\n"
          ]
        }
      ],
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epdW8u_YXcAv"
      },
      "source": [
        "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
        "\n",
        "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTai8Ta0lgwL",
        "outputId": "429fe044-ead2-423d-87da-1c91ce2662d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words count in train set: 739769\n",
            "Words count in val set: 130954\n",
            "Words count in test set: 290469\n"
          ]
        }
      ],
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n",
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eChdLNGtXyP0"
      },
      "source": [
        "Построим маппинги из слов в индекс и из тега в индекс:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCjwwDs6Zq9x",
        "outputId": "21a84566-4169-4161-ab25-1dc13e1a8c57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique words in train = 45441. Tags = {'NUM', 'ADJ', '.', 'CONJ', 'ADV', 'PRON', 'PRT', 'ADP', 'X', 'VERB', 'DET', 'NOUN'}\n"
          ]
        }
      ],
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "URC1B2nvPGFt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "d4a3c2a1-6200-42b9-d3de-e83b22112e03"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "    \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gArQwbzWWkgi"
      },
      "source": [
        "## Бейзлайн\n",
        "\n",
        "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
        "\n",
        "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
        "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
        "\n",
        "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
        "\n",
        "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
        "\n",
        "Простейший вариант - униграммная модель, учитывающая только слово:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5rWmSToIaeAo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b90a3ee-36f0-46d8-8e4f-2c75d9819b88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of unigram tagger = 92.62%\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "\n",
        "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
        "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Ymb_MkbWsF"
      },
      "source": [
        "Добавим вероятности переходов:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vjz_Rk0bbMyH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3658b9b6-e316-4d2f-a240-89e9a5027b05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of bigram tagger = 93.42%\n"
          ]
        }
      ],
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWMw6QHvbaDd"
      },
      "source": [
        "Обратите внимание, что `backoff` важен:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8XCuxEBVbOY_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "497b51f9-7375-4715-e2f9-918a596ef3bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of trigram tagger = 23.33%\n"
          ]
        }
      ],
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t3xyYd__8d-"
      },
      "source": [
        "## Увеличиваем контекст с рекуррентными сетями\n",
        "\n",
        "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
        "\n",
        "Омонимия - основная причина, почему униграмная модель плоха:  \n",
        "*“he cashed a check at the **bank**”*  \n",
        "vs  \n",
        "*“he sat on the **bank** of the river”*\n",
        "\n",
        "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
        "\n",
        "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
        "\n",
        "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
        "\n",
        "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RtRbz1SwgEqc"
      },
      "outputs": [],
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DhsTKZalfih6"
      },
      "outputs": [],
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_indices = indices[start:end]\n",
        "        \n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        \n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "            \n",
        "        yield X_batch, y_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "l4XsRII5kW5x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62a888fc-f695-40e9-a716-99d247cf35fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 4), (32, 4))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5I9E9P6eFYv"
      },
      "source": [
        "**Задание** Реализуйте `LSTMTagger`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WVEHju54d68T"
      },
      "outputs": [],
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._emb = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self._lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count)\n",
        "        self._out_layer = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        emb = self._emb(inputs)\n",
        "        out, _ = self._lstm(emb)\n",
        "        return self._out_layer(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_HA8zyheYGH"
      },
      "source": [
        "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jbrxsZ2mehWB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5515fce-ed04-4450-d6c6-307903452d0c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.06521739130434782"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
        "\n",
        "logits = model(X_batch)\n",
        "\n",
        "preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "mask = (y_batch != 0).float()\n",
        "correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
        "total_count = mask.sum().item()\n",
        "\n",
        "correct_count / total_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GMUyUm1hgpe3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0699f73-4d8a-4e0d-af7d-3a8048083c9d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.6038, grad_fn=<NllLoss2DBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion(logits.transpose(2, 1), y_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSgV3NPUpcjH"
      },
      "source": [
        "**Задание** Вставьте эти вычисление в функцию:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FprPQ0gllo7b"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "\n",
        "                loss = criterion(logits.view(-1, logits.shape[-1]), y_batch.view(-1))\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "                mask = (y_batch != 0).float()\n",
        "                cur_correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
        "                cur_sum_count = mask.sum().item()\n",
        "                \n",
        "                # cur_correct_count, cur_sum_count = <calc accuracy>\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Pqfbeh1ltEYa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbac29da-a4c8-4a46-939c-b859c71086df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[1 / 50] Train: Loss = 0.31457, Accuracy = 72.14%: 100%|██████████| 572/572 [00:18<00:00, 31.38it/s]\n",
            "[1 / 50]   Val: Loss = 0.10366, Accuracy = 85.23%: 100%|██████████| 13/13 [00:00<00:00, 37.61it/s]\n",
            "[2 / 50] Train: Loss = 0.10022, Accuracy = 90.08%: 100%|██████████| 572/572 [00:15<00:00, 37.99it/s]\n",
            "[2 / 50]   Val: Loss = 0.07477, Accuracy = 89.58%: 100%|██████████| 13/13 [00:00<00:00, 36.32it/s]\n",
            "[3 / 50] Train: Loss = 0.06770, Accuracy = 93.25%: 100%|██████████| 572/572 [00:15<00:00, 37.91it/s]\n",
            "[3 / 50]   Val: Loss = 0.06763, Accuracy = 91.24%: 100%|██████████| 13/13 [00:00<00:00, 37.64it/s]\n",
            "[4 / 50] Train: Loss = 0.05083, Accuracy = 94.85%: 100%|██████████| 572/572 [00:15<00:00, 37.65it/s]\n",
            "[4 / 50]   Val: Loss = 0.06545, Accuracy = 92.17%: 100%|██████████| 13/13 [00:00<00:00, 36.56it/s]\n",
            "[5 / 50] Train: Loss = 0.04079, Accuracy = 95.84%: 100%|██████████| 572/572 [00:15<00:00, 37.89it/s]\n",
            "[5 / 50]   Val: Loss = 0.06670, Accuracy = 92.69%: 100%|██████████| 13/13 [00:00<00:00, 37.52it/s]\n",
            "[6 / 50] Train: Loss = 0.03322, Accuracy = 96.56%: 100%|██████████| 572/572 [00:15<00:00, 36.39it/s]\n",
            "[6 / 50]   Val: Loss = 0.06270, Accuracy = 93.00%: 100%|██████████| 13/13 [00:00<00:00, 36.41it/s]\n",
            "[7 / 50] Train: Loss = 0.02762, Accuracy = 97.14%: 100%|██████████| 572/572 [00:15<00:00, 37.89it/s]\n",
            "[7 / 50]   Val: Loss = 0.06474, Accuracy = 93.22%: 100%|██████████| 13/13 [00:00<00:00, 37.29it/s]\n",
            "[8 / 50] Train: Loss = 0.02279, Accuracy = 97.62%: 100%|██████████| 572/572 [00:15<00:00, 37.63it/s]\n",
            "[8 / 50]   Val: Loss = 0.06554, Accuracy = 93.20%: 100%|██████████| 13/13 [00:00<00:00, 35.98it/s]\n",
            "[9 / 50] Train: Loss = 0.01901, Accuracy = 98.00%: 100%|██████████| 572/572 [00:15<00:00, 37.79it/s]\n",
            "[9 / 50]   Val: Loss = 0.07233, Accuracy = 93.32%: 100%|██████████| 13/13 [00:00<00:00, 39.02it/s]\n",
            "[10 / 50] Train: Loss = 0.01599, Accuracy = 98.34%: 100%|██████████| 572/572 [00:15<00:00, 37.81it/s]\n",
            "[10 / 50]   Val: Loss = 0.07634, Accuracy = 93.31%: 100%|██████████| 13/13 [00:00<00:00, 39.24it/s]\n",
            "[11 / 50] Train: Loss = 0.01326, Accuracy = 98.64%: 100%|██████████| 572/572 [00:15<00:00, 37.57it/s]\n",
            "[11 / 50]   Val: Loss = 0.06865, Accuracy = 93.33%: 100%|██████████| 13/13 [00:00<00:00, 35.58it/s]\n",
            "[12 / 50] Train: Loss = 0.01094, Accuracy = 98.90%: 100%|██████████| 572/572 [00:15<00:00, 37.60it/s]\n",
            "[12 / 50]   Val: Loss = 0.07806, Accuracy = 93.28%: 100%|██████████| 13/13 [00:00<00:00, 37.20it/s]\n",
            "[13 / 50] Train: Loss = 0.00893, Accuracy = 99.11%: 100%|██████████| 572/572 [00:15<00:00, 37.76it/s]\n",
            "[13 / 50]   Val: Loss = 0.07458, Accuracy = 93.23%: 100%|██████████| 13/13 [00:00<00:00, 35.61it/s]\n",
            "[14 / 50] Train: Loss = 0.00736, Accuracy = 99.28%: 100%|██████████| 572/572 [00:15<00:00, 37.80it/s]\n",
            "[14 / 50]   Val: Loss = 0.07631, Accuracy = 93.15%: 100%|██████████| 13/13 [00:00<00:00, 35.53it/s]\n",
            "[15 / 50] Train: Loss = 0.00605, Accuracy = 99.42%: 100%|██████████| 572/572 [00:15<00:00, 37.77it/s]\n",
            "[15 / 50]   Val: Loss = 0.08574, Accuracy = 93.20%: 100%|██████████| 13/13 [00:00<00:00, 36.10it/s]\n",
            "[16 / 50] Train: Loss = 0.00483, Accuracy = 99.55%: 100%|██████████| 572/572 [00:15<00:00, 37.69it/s]\n",
            "[16 / 50]   Val: Loss = 0.08787, Accuracy = 93.12%: 100%|██████████| 13/13 [00:00<00:00, 35.65it/s]\n",
            "[17 / 50] Train: Loss = 0.00397, Accuracy = 99.64%: 100%|██████████| 572/572 [00:15<00:00, 37.79it/s]\n",
            "[17 / 50]   Val: Loss = 0.09140, Accuracy = 93.08%: 100%|██████████| 13/13 [00:00<00:00, 34.84it/s]\n",
            "[18 / 50] Train: Loss = 0.00338, Accuracy = 99.69%: 100%|██████████| 572/572 [00:15<00:00, 37.35it/s]\n",
            "[18 / 50]   Val: Loss = 0.10242, Accuracy = 93.06%: 100%|██████████| 13/13 [00:00<00:00, 37.80it/s]\n",
            "[19 / 50] Train: Loss = 0.00283, Accuracy = 99.75%: 100%|██████████| 572/572 [00:15<00:00, 36.93it/s]\n",
            "[19 / 50]   Val: Loss = 0.10169, Accuracy = 93.03%: 100%|██████████| 13/13 [00:00<00:00, 35.86it/s]\n",
            "[20 / 50] Train: Loss = 0.00255, Accuracy = 99.77%: 100%|██████████| 572/572 [00:15<00:00, 37.33it/s]\n",
            "[20 / 50]   Val: Loss = 0.10805, Accuracy = 92.96%: 100%|██████████| 13/13 [00:00<00:00, 36.92it/s]\n",
            "[21 / 50] Train: Loss = 0.00228, Accuracy = 99.79%: 100%|██████████| 572/572 [00:15<00:00, 37.05it/s]\n",
            "[21 / 50]   Val: Loss = 0.10578, Accuracy = 93.01%: 100%|██████████| 13/13 [00:00<00:00, 34.67it/s]\n",
            "[22 / 50] Train: Loss = 0.00199, Accuracy = 99.81%: 100%|██████████| 572/572 [00:15<00:00, 37.10it/s]\n",
            "[22 / 50]   Val: Loss = 0.11788, Accuracy = 92.99%: 100%|██████████| 13/13 [00:00<00:00, 38.24it/s]\n",
            "[23 / 50] Train: Loss = 0.00196, Accuracy = 99.81%: 100%|██████████| 572/572 [00:15<00:00, 36.88it/s]\n",
            "[23 / 50]   Val: Loss = 0.11314, Accuracy = 92.97%: 100%|██████████| 13/13 [00:00<00:00, 36.58it/s]\n",
            "[24 / 50] Train: Loss = 0.00190, Accuracy = 99.81%: 100%|██████████| 572/572 [00:15<00:00, 37.31it/s]\n",
            "[24 / 50]   Val: Loss = 0.12761, Accuracy = 92.98%: 100%|██████████| 13/13 [00:00<00:00, 37.78it/s]\n",
            "[25 / 50] Train: Loss = 0.00197, Accuracy = 99.80%: 100%|██████████| 572/572 [00:15<00:00, 37.36it/s]\n",
            "[25 / 50]   Val: Loss = 0.11448, Accuracy = 93.05%: 100%|██████████| 13/13 [00:00<00:00, 35.68it/s]\n",
            "[26 / 50] Train: Loss = 0.00197, Accuracy = 99.79%: 100%|██████████| 572/572 [00:15<00:00, 37.35it/s]\n",
            "[26 / 50]   Val: Loss = 0.12248, Accuracy = 92.99%: 100%|██████████| 13/13 [00:00<00:00, 36.60it/s]\n",
            "[27 / 50] Train: Loss = 0.00170, Accuracy = 99.82%: 100%|██████████| 572/572 [00:15<00:00, 37.34it/s]\n",
            "[27 / 50]   Val: Loss = 0.12970, Accuracy = 92.98%: 100%|██████████| 13/13 [00:00<00:00, 36.63it/s]\n",
            "[28 / 50] Train: Loss = 0.00168, Accuracy = 99.82%: 100%|██████████| 572/572 [00:15<00:00, 36.90it/s]\n",
            "[28 / 50]   Val: Loss = 0.12420, Accuracy = 93.03%: 100%|██████████| 13/13 [00:00<00:00, 34.64it/s]\n",
            "[29 / 50] Train: Loss = 0.00155, Accuracy = 99.83%: 100%|██████████| 572/572 [00:15<00:00, 36.37it/s]\n",
            "[29 / 50]   Val: Loss = 0.13142, Accuracy = 93.00%: 100%|██████████| 13/13 [00:00<00:00, 35.11it/s]\n",
            "[30 / 50] Train: Loss = 0.00164, Accuracy = 99.83%: 100%|██████████| 572/572 [00:15<00:00, 36.75it/s]\n",
            "[30 / 50]   Val: Loss = 0.13468, Accuracy = 93.00%: 100%|██████████| 13/13 [00:00<00:00, 35.70it/s]\n",
            "[31 / 50] Train: Loss = 0.00155, Accuracy = 99.83%: 100%|██████████| 572/572 [00:15<00:00, 35.97it/s]\n",
            "[31 / 50]   Val: Loss = 0.13529, Accuracy = 93.03%: 100%|██████████| 13/13 [00:00<00:00, 26.30it/s]\n",
            "[32 / 50] Train: Loss = 0.00158, Accuracy = 99.83%: 100%|██████████| 572/572 [00:17<00:00, 32.19it/s]\n",
            "[32 / 50]   Val: Loss = 0.13154, Accuracy = 93.08%: 100%|██████████| 13/13 [00:00<00:00, 37.50it/s]\n",
            "[33 / 50] Train: Loss = 0.00181, Accuracy = 99.80%: 100%|██████████| 572/572 [00:15<00:00, 37.08it/s]\n",
            "[33 / 50]   Val: Loss = 0.13703, Accuracy = 93.00%: 100%|██████████| 13/13 [00:00<00:00, 36.97it/s]\n",
            "[34 / 50] Train: Loss = 0.00157, Accuracy = 99.83%: 100%|██████████| 572/572 [00:15<00:00, 36.66it/s]\n",
            "[34 / 50]   Val: Loss = 0.14633, Accuracy = 93.07%: 100%|██████████| 13/13 [00:00<00:00, 37.19it/s]\n",
            "[35 / 50] Train: Loss = 0.00145, Accuracy = 99.83%: 100%|██████████| 572/572 [00:15<00:00, 37.20it/s]\n",
            "[35 / 50]   Val: Loss = 0.13416, Accuracy = 93.08%: 100%|██████████| 13/13 [00:00<00:00, 36.03it/s]\n",
            "[36 / 50] Train: Loss = 0.00134, Accuracy = 99.84%: 100%|██████████| 572/572 [00:15<00:00, 37.03it/s]\n",
            "[36 / 50]   Val: Loss = 0.15389, Accuracy = 93.03%: 100%|██████████| 13/13 [00:00<00:00, 36.17it/s]\n",
            "[37 / 50] Train: Loss = 0.00138, Accuracy = 99.84%: 100%|██████████| 572/572 [00:15<00:00, 37.18it/s]\n",
            "[37 / 50]   Val: Loss = 0.15230, Accuracy = 93.11%: 100%|██████████| 13/13 [00:00<00:00, 36.74it/s]\n",
            "[38 / 50] Train: Loss = 0.00134, Accuracy = 99.84%: 100%|██████████| 572/572 [00:15<00:00, 37.03it/s]\n",
            "[38 / 50]   Val: Loss = 0.13771, Accuracy = 93.10%: 100%|██████████| 13/13 [00:00<00:00, 35.00it/s]\n",
            "[39 / 50] Train: Loss = 0.00204, Accuracy = 99.76%: 100%|██████████| 572/572 [00:15<00:00, 36.98it/s]\n",
            "[39 / 50]   Val: Loss = 0.16297, Accuracy = 93.02%: 100%|██████████| 13/13 [00:00<00:00, 38.84it/s]\n",
            "[40 / 50] Train: Loss = 0.00164, Accuracy = 99.81%: 100%|██████████| 572/572 [00:15<00:00, 37.29it/s]\n",
            "[40 / 50]   Val: Loss = 0.15721, Accuracy = 93.01%: 100%|██████████| 13/13 [00:00<00:00, 36.30it/s]\n",
            "[41 / 50] Train: Loss = 0.00134, Accuracy = 99.84%: 100%|██████████| 572/572 [00:15<00:00, 37.12it/s]\n",
            "[41 / 50]   Val: Loss = 0.15413, Accuracy = 93.07%: 100%|██████████| 13/13 [00:00<00:00, 36.33it/s]\n",
            "[42 / 50] Train: Loss = 0.00128, Accuracy = 99.85%: 100%|██████████| 572/572 [00:15<00:00, 37.35it/s]\n",
            "[42 / 50]   Val: Loss = 0.17340, Accuracy = 93.07%: 100%|██████████| 13/13 [00:00<00:00, 38.99it/s]\n",
            "[43 / 50] Train: Loss = 0.00127, Accuracy = 99.84%: 100%|██████████| 572/572 [00:15<00:00, 37.22it/s]\n",
            "[43 / 50]   Val: Loss = 0.16249, Accuracy = 93.10%: 100%|██████████| 13/13 [00:00<00:00, 37.55it/s]\n",
            "[44 / 50] Train: Loss = 0.00126, Accuracy = 99.85%: 100%|██████████| 572/572 [00:15<00:00, 37.35it/s]\n",
            "[44 / 50]   Val: Loss = 0.16066, Accuracy = 93.07%: 100%|██████████| 13/13 [00:00<00:00, 37.95it/s]\n",
            "[45 / 50] Train: Loss = 0.00167, Accuracy = 99.80%: 100%|██████████| 572/572 [00:15<00:00, 37.22it/s]\n",
            "[45 / 50]   Val: Loss = 0.15857, Accuracy = 92.87%: 100%|██████████| 13/13 [00:00<00:00, 35.58it/s]\n",
            "[46 / 50] Train: Loss = 0.00211, Accuracy = 99.76%: 100%|██████████| 572/572 [00:15<00:00, 36.39it/s]\n",
            "[46 / 50]   Val: Loss = 0.16286, Accuracy = 92.91%: 100%|██████████| 13/13 [00:00<00:00, 36.22it/s]\n",
            "[47 / 50] Train: Loss = 0.00137, Accuracy = 99.84%: 100%|██████████| 572/572 [00:15<00:00, 37.23it/s]\n",
            "[47 / 50]   Val: Loss = 0.15856, Accuracy = 93.02%: 100%|██████████| 13/13 [00:00<00:00, 36.29it/s]\n",
            "[48 / 50] Train: Loss = 0.00123, Accuracy = 99.85%: 100%|██████████| 572/572 [00:15<00:00, 37.22it/s]\n",
            "[48 / 50]   Val: Loss = 0.16510, Accuracy = 93.06%: 100%|██████████| 13/13 [00:00<00:00, 37.50it/s]\n",
            "[49 / 50] Train: Loss = 0.00123, Accuracy = 99.85%: 100%|██████████| 572/572 [00:15<00:00, 37.18it/s]\n",
            "[49 / 50]   Val: Loss = 0.16554, Accuracy = 93.04%: 100%|██████████| 13/13 [00:00<00:00, 36.44it/s]\n",
            "[50 / 50] Train: Loss = 0.00121, Accuracy = 99.85%: 100%|██████████| 572/572 [00:15<00:00, 36.98it/s]\n",
            "[50 / 50]   Val: Loss = 0.17181, Accuracy = 93.07%: 100%|██████████| 13/13 [00:00<00:00, 36.66it/s]\n"
          ]
        }
      ],
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0qGetIhfUE5"
      },
      "source": [
        "### Masking\n",
        "\n",
        "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
        "\n",
        "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAfV2dEOfHo5"
      },
      "source": [
        "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "98wr38_rw55D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c8f6c6-0d28-4016-d770-56f77aa59066"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9315142063352716"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "correct_count = 0\n",
        "sum_count = 0\n",
        "\n",
        "for X_batch, y_batch in iterate_batches((X_test, y_test), 64):\n",
        "    X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "    logits = model(X_batch)\n",
        "\n",
        "    preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    mask = (y_batch != 0).float()\n",
        "    cur_correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
        "    cur_sum_count = mask.sum().item()\n",
        "                \n",
        "    correct_count += cur_correct_count\n",
        "    sum_count += cur_sum_count\n",
        "\n",
        "correct_count / sum_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXUTSFaEHbDG"
      },
      "source": [
        "### Bidirectional LSTM\n",
        "\n",
        "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
        "\n",
        "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
        "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
        "\n",
        "**Задание** Добавьте Bidirectional LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "NXNqS6loSNcZ"
      },
      "outputs": [],
      "source": [
        "class BidirectionalLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._emb = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self._lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count, bidirectional=True)\n",
        "        self._out_layer = nn.Linear(lstm_hidden_dim * 2, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        emb = self._emb(inputs)\n",
        "        out, _ = self._lstm(emb)\n",
        "        return self._out_layer(out)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BidirectionalLSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hISc7YfzbcHJ",
        "outputId": "e61f0ddf-2a3e-4a16-8bea-93839c7becfb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[1 / 50] Train: Loss = 0.25669, Accuracy = 76.72%: 100%|██████████| 572/572 [00:17<00:00, 33.37it/s]\n",
            "[1 / 50]   Val: Loss = 0.07118, Accuracy = 89.41%: 100%|██████████| 13/13 [00:00<00:00, 27.42it/s]\n",
            "[2 / 50] Train: Loss = 0.07620, Accuracy = 92.52%: 100%|██████████| 572/572 [00:17<00:00, 33.56it/s]\n",
            "[2 / 50]   Val: Loss = 0.04686, Accuracy = 93.41%: 100%|██████████| 13/13 [00:00<00:00, 27.36it/s]\n",
            "[3 / 50] Train: Loss = 0.04945, Accuracy = 95.32%: 100%|██████████| 572/572 [00:16<00:00, 33.67it/s]\n",
            "[3 / 50]   Val: Loss = 0.03993, Accuracy = 94.81%: 100%|██████████| 13/13 [00:00<00:00, 28.36it/s]\n",
            "[4 / 50] Train: Loss = 0.03488, Accuracy = 96.72%: 100%|██████████| 572/572 [00:16<00:00, 33.69it/s]\n",
            "[4 / 50]   Val: Loss = 0.03550, Accuracy = 95.52%: 100%|██████████| 13/13 [00:00<00:00, 28.68it/s]\n",
            "[5 / 50] Train: Loss = 0.02590, Accuracy = 97.62%: 100%|██████████| 572/572 [00:16<00:00, 33.80it/s]\n",
            "[5 / 50]   Val: Loss = 0.03443, Accuracy = 95.83%: 100%|██████████| 13/13 [00:00<00:00, 28.59it/s]\n",
            "[6 / 50] Train: Loss = 0.01893, Accuracy = 98.27%: 100%|██████████| 572/572 [00:17<00:00, 33.49it/s]\n",
            "[6 / 50]   Val: Loss = 0.03409, Accuracy = 96.11%: 100%|██████████| 13/13 [00:00<00:00, 27.70it/s]\n",
            "[7 / 50] Train: Loss = 0.01373, Accuracy = 98.75%: 100%|██████████| 572/572 [00:16<00:00, 33.71it/s]\n",
            "[7 / 50]   Val: Loss = 0.03408, Accuracy = 96.17%: 100%|██████████| 13/13 [00:00<00:00, 27.84it/s]\n",
            "[8 / 50] Train: Loss = 0.00981, Accuracy = 99.15%: 100%|██████████| 572/572 [00:16<00:00, 33.92it/s]\n",
            "[8 / 50]   Val: Loss = 0.03358, Accuracy = 96.19%: 100%|██████████| 13/13 [00:00<00:00, 26.82it/s]\n",
            "[9 / 50] Train: Loss = 0.00677, Accuracy = 99.43%: 100%|██████████| 572/572 [00:17<00:00, 33.60it/s]\n",
            "[9 / 50]   Val: Loss = 0.03638, Accuracy = 96.34%: 100%|██████████| 13/13 [00:00<00:00, 28.02it/s]\n",
            "[10 / 50] Train: Loss = 0.00456, Accuracy = 99.64%: 100%|██████████| 572/572 [00:16<00:00, 33.76it/s]\n",
            "[10 / 50]   Val: Loss = 0.03648, Accuracy = 96.34%: 100%|██████████| 13/13 [00:00<00:00, 27.85it/s]\n",
            "[11 / 50] Train: Loss = 0.00294, Accuracy = 99.79%: 100%|██████████| 572/572 [00:17<00:00, 33.57it/s]\n",
            "[11 / 50]   Val: Loss = 0.03751, Accuracy = 96.28%: 100%|██████████| 13/13 [00:00<00:00, 26.87it/s]\n",
            "[12 / 50] Train: Loss = 0.00187, Accuracy = 99.89%: 100%|██████████| 572/572 [00:17<00:00, 33.41it/s]\n",
            "[12 / 50]   Val: Loss = 0.04245, Accuracy = 96.37%: 100%|██████████| 13/13 [00:00<00:00, 29.06it/s]\n",
            "[13 / 50] Train: Loss = 0.00112, Accuracy = 99.95%: 100%|██████████| 572/572 [00:16<00:00, 33.90it/s]\n",
            "[13 / 50]   Val: Loss = 0.04027, Accuracy = 96.38%: 100%|██████████| 13/13 [00:00<00:00, 26.93it/s]\n",
            "[14 / 50] Train: Loss = 0.00073, Accuracy = 99.97%: 100%|██████████| 572/572 [00:16<00:00, 34.06it/s]\n",
            "[14 / 50]   Val: Loss = 0.04466, Accuracy = 96.40%: 100%|██████████| 13/13 [00:00<00:00, 26.49it/s]\n",
            "[15 / 50] Train: Loss = 0.00067, Accuracy = 99.97%: 100%|██████████| 572/572 [00:16<00:00, 33.70it/s]\n",
            "[15 / 50]   Val: Loss = 0.04913, Accuracy = 96.33%: 100%|██████████| 13/13 [00:00<00:00, 27.82it/s]\n",
            "[16 / 50] Train: Loss = 0.00067, Accuracy = 99.96%: 100%|██████████| 572/572 [00:17<00:00, 33.51it/s]\n",
            "[16 / 50]   Val: Loss = 0.04728, Accuracy = 96.36%: 100%|██████████| 13/13 [00:00<00:00, 27.11it/s]\n",
            "[17 / 50] Train: Loss = 0.00042, Accuracy = 99.98%: 100%|██████████| 572/572 [00:16<00:00, 33.71it/s]\n",
            "[17 / 50]   Val: Loss = 0.05074, Accuracy = 96.40%: 100%|██████████| 13/13 [00:00<00:00, 28.35it/s]\n",
            "[18 / 50] Train: Loss = 0.00027, Accuracy = 99.99%: 100%|██████████| 572/572 [00:16<00:00, 33.80it/s]\n",
            "[18 / 50]   Val: Loss = 0.05026, Accuracy = 96.41%: 100%|██████████| 13/13 [00:00<00:00, 27.00it/s]\n",
            "[19 / 50] Train: Loss = 0.00042, Accuracy = 99.97%: 100%|██████████| 572/572 [00:17<00:00, 33.54it/s]\n",
            "[19 / 50]   Val: Loss = 0.05018, Accuracy = 96.25%: 100%|██████████| 13/13 [00:00<00:00, 26.19it/s]\n",
            "[20 / 50] Train: Loss = 0.00091, Accuracy = 99.92%: 100%|██████████| 572/572 [00:17<00:00, 33.64it/s]\n",
            "[20 / 50]   Val: Loss = 0.05306, Accuracy = 96.28%: 100%|██████████| 13/13 [00:00<00:00, 28.68it/s]\n",
            "[21 / 50] Train: Loss = 0.00029, Accuracy = 99.98%: 100%|██████████| 572/572 [00:16<00:00, 33.75it/s]\n",
            "[21 / 50]   Val: Loss = 0.05527, Accuracy = 96.42%: 100%|██████████| 13/13 [00:00<00:00, 28.44it/s]\n",
            "[22 / 50] Train: Loss = 0.00010, Accuracy = 100.00%: 100%|██████████| 572/572 [00:17<00:00, 33.57it/s]\n",
            "[22 / 50]   Val: Loss = 0.05450, Accuracy = 96.39%: 100%|██████████| 13/13 [00:00<00:00, 27.09it/s]\n",
            "[23 / 50] Train: Loss = 0.00007, Accuracy = 100.00%: 100%|██████████| 572/572 [00:17<00:00, 33.63it/s]\n",
            "[23 / 50]   Val: Loss = 0.05771, Accuracy = 96.46%: 100%|██████████| 13/13 [00:00<00:00, 28.54it/s]\n",
            "[24 / 50] Train: Loss = 0.00005, Accuracy = 100.00%: 100%|██████████| 572/572 [00:16<00:00, 33.79it/s]\n",
            "[24 / 50]   Val: Loss = 0.05497, Accuracy = 96.47%: 100%|██████████| 13/13 [00:00<00:00, 27.47it/s]\n",
            "[25 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:17<00:00, 33.53it/s]\n",
            "[25 / 50]   Val: Loss = 0.05554, Accuracy = 96.44%: 100%|██████████| 13/13 [00:00<00:00, 26.91it/s]\n",
            "[26 / 50] Train: Loss = 0.00151, Accuracy = 99.85%: 100%|██████████| 572/572 [00:16<00:00, 33.79it/s]\n",
            "[26 / 50]   Val: Loss = 0.05855, Accuracy = 96.22%: 100%|██████████| 13/13 [00:00<00:00, 27.91it/s]\n",
            "[27 / 50] Train: Loss = 0.00057, Accuracy = 99.95%: 100%|██████████| 572/572 [00:17<00:00, 33.38it/s]\n",
            "[27 / 50]   Val: Loss = 0.05888, Accuracy = 96.19%: 100%|██████████| 13/13 [00:00<00:00, 28.27it/s]\n",
            "[28 / 50] Train: Loss = 0.00010, Accuracy = 100.00%: 100%|██████████| 572/572 [00:17<00:00, 33.53it/s]\n",
            "[28 / 50]   Val: Loss = 0.05842, Accuracy = 96.29%: 100%|██████████| 13/13 [00:00<00:00, 28.57it/s]\n",
            "[29 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:17<00:00, 33.61it/s]\n",
            "[29 / 50]   Val: Loss = 0.05519, Accuracy = 96.30%: 100%|██████████| 13/13 [00:00<00:00, 26.64it/s]\n",
            "[30 / 50] Train: Loss = 0.00003, Accuracy = 100.00%: 100%|██████████| 572/572 [00:17<00:00, 33.41it/s]\n",
            "[30 / 50]   Val: Loss = 0.05905, Accuracy = 96.32%: 100%|██████████| 13/13 [00:00<00:00, 26.90it/s]\n",
            "[31 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:17<00:00, 33.01it/s]\n",
            "[31 / 50]   Val: Loss = 0.06293, Accuracy = 96.31%: 100%|██████████| 13/13 [00:00<00:00, 27.95it/s]\n",
            "[32 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:17<00:00, 32.48it/s]\n",
            "[32 / 50]   Val: Loss = 0.05932, Accuracy = 96.32%: 100%|██████████| 13/13 [00:00<00:00, 26.75it/s]\n",
            "[33 / 50] Train: Loss = 0.00001, Accuracy = 100.00%: 100%|██████████| 572/572 [00:17<00:00, 32.76it/s]\n",
            "[33 / 50]   Val: Loss = 0.06695, Accuracy = 96.35%: 100%|██████████| 13/13 [00:00<00:00, 27.19it/s]\n",
            "[34 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:17<00:00, 32.87it/s]\n",
            "[34 / 50]   Val: Loss = 0.06069, Accuracy = 96.31%: 100%|██████████| 13/13 [00:00<00:00, 27.33it/s]\n",
            "[35 / 50] Train: Loss = 0.00056, Accuracy = 99.95%: 100%|██████████| 572/572 [00:17<00:00, 33.13it/s]\n",
            "[35 / 50]   Val: Loss = 0.06757, Accuracy = 95.79%: 100%|██████████| 13/13 [00:00<00:00, 26.73it/s]\n",
            "[36 / 50] Train: Loss = 0.00177, Accuracy = 99.82%: 100%|██████████| 572/572 [00:17<00:00, 33.12it/s]\n",
            "[36 / 50]   Val: Loss = 0.06040, Accuracy = 95.85%: 100%|██████████| 13/13 [00:00<00:00, 25.74it/s]\n",
            "[37 / 50] Train: Loss = 0.00021, Accuracy = 99.98%: 100%|██████████| 572/572 [00:17<00:00, 32.96it/s]\n",
            "[37 / 50]   Val: Loss = 0.05904, Accuracy = 96.20%: 100%|██████████| 13/13 [00:00<00:00, 26.48it/s]\n",
            "[38 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:17<00:00, 33.43it/s]\n",
            "[38 / 50]   Val: Loss = 0.06267, Accuracy = 96.25%: 100%|██████████| 13/13 [00:00<00:00, 28.26it/s]\n",
            "[39 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:17<00:00, 33.31it/s]\n",
            "[39 / 50]   Val: Loss = 0.06195, Accuracy = 96.27%: 100%|██████████| 13/13 [00:00<00:00, 27.53it/s]\n",
            "[40 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:17<00:00, 33.11it/s]\n",
            "[40 / 50]   Val: Loss = 0.06053, Accuracy = 96.28%: 100%|██████████| 13/13 [00:00<00:00, 26.23it/s]\n",
            "[41 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:17<00:00, 33.37it/s]\n",
            "[41 / 50]   Val: Loss = 0.06182, Accuracy = 96.28%: 100%|██████████| 13/13 [00:00<00:00, 27.14it/s]\n",
            "[42 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:17<00:00, 33.41it/s]\n",
            "[42 / 50]   Val: Loss = 0.06634, Accuracy = 96.32%: 100%|██████████| 13/13 [00:00<00:00, 28.36it/s]\n",
            "[43 / 50] Train: Loss = 0.00001, Accuracy = 100.00%: 100%|██████████| 572/572 [00:17<00:00, 33.36it/s]\n",
            "[43 / 50]   Val: Loss = 0.06644, Accuracy = 96.29%: 100%|██████████| 13/13 [00:00<00:00, 27.00it/s]\n",
            "[44 / 50] Train: Loss = 0.00001, Accuracy = 100.00%: 100%|██████████| 572/572 [00:17<00:00, 33.36it/s]\n",
            "[44 / 50]   Val: Loss = 0.06533, Accuracy = 96.31%: 100%|██████████| 13/13 [00:00<00:00, 27.51it/s]\n",
            "[45 / 50] Train: Loss = 0.00001, Accuracy = 100.00%: 100%|██████████| 572/572 [00:17<00:00, 33.39it/s]\n",
            "[45 / 50]   Val: Loss = 0.07028, Accuracy = 96.31%: 100%|██████████| 13/13 [00:00<00:00, 29.32it/s]\n",
            "[46 / 50] Train: Loss = 0.00001, Accuracy = 100.00%: 100%|██████████| 572/572 [00:17<00:00, 33.29it/s]\n",
            "[46 / 50]   Val: Loss = 0.06869, Accuracy = 96.31%: 100%|██████████| 13/13 [00:00<00:00, 27.74it/s]\n",
            "[47 / 50] Train: Loss = 0.00008, Accuracy = 100.00%: 100%|██████████| 572/572 [00:17<00:00, 32.75it/s]\n",
            "[47 / 50]   Val: Loss = 0.07723, Accuracy = 95.91%: 100%|██████████| 13/13 [00:00<00:00, 28.01it/s]\n",
            "[48 / 50] Train: Loss = 0.00179, Accuracy = 99.81%: 100%|██████████| 572/572 [00:17<00:00, 33.22it/s]\n",
            "[48 / 50]   Val: Loss = 0.06266, Accuracy = 96.29%: 100%|██████████| 13/13 [00:00<00:00, 26.70it/s]\n",
            "[49 / 50] Train: Loss = 0.00023, Accuracy = 99.98%: 100%|██████████| 572/572 [00:17<00:00, 33.43it/s]\n",
            "[49 / 50]   Val: Loss = 0.07039, Accuracy = 96.31%: 100%|██████████| 13/13 [00:00<00:00, 27.37it/s]\n",
            "[50 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:17<00:00, 33.37it/s]\n",
            "[50 / 50]   Val: Loss = 0.06432, Accuracy = 96.37%: 100%|██████████| 13/13 [00:00<00:00, 27.73it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct_count = 0\n",
        "sum_count = 0\n",
        "\n",
        "for X_batch, y_batch in iterate_batches((X_test, y_test), 64):\n",
        "    X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "    logits = model(X_batch)\n",
        "\n",
        "    preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    mask = (y_batch != 0).float()\n",
        "    cur_correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
        "    cur_sum_count = mask.sum().item()\n",
        "                \n",
        "    correct_count += cur_correct_count\n",
        "    sum_count += cur_sum_count\n",
        "\n",
        "correct_count / sum_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHCt65uGbeuZ",
        "outputId": "85c3f317-877e-4158-bb2e-31001ab6880e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9638550069026299"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTXmYGD_ANhm"
      },
      "source": [
        "### Предобученные эмбеддинги\n",
        "\n",
        "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
        "\n",
        "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "uZpY_Q1xZ18h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcd61a8c-1277-4c91-daeb-ff6ddb971eba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYogOoKlgtcf"
      },
      "source": [
        "Построим подматрицу для слов из нашей тренировочной выборки:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "VsCstxiO03oT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a619c62-bff7-47ad-e0f7-2bf64bd82a0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Know 38736 out of 45441 word embeddings\n"
          ]
        }
      ],
      "source": [
        "known_count = 0\n",
        "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
        "for word, ind in word2ind.items():\n",
        "    word = word.lower()\n",
        "    if word in w2v_model.vocab:\n",
        "        embeddings[ind] = w2v_model.get_vector(word)\n",
        "        known_count += 1\n",
        "        \n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcG7i-R8hbY3"
      },
      "source": [
        "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "LxaRBpQd0pat"
      },
      "outputs": [],
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        embeddings = torch.FloatTensor(embeddings)\n",
        "        self._emb =nn.Embedding.from_pretrained(embeddings)\n",
        "        self._lstm = nn.LSTM(embeddings.shape[1], lstm_hidden_dim, num_layers=lstm_layers_count)\n",
        "        self._out_layer = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \n",
        "        emb = self._emb(inputs)\n",
        "        out, _ = self._lstm(emb)\n",
        "        return self._out_layer(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "EBtI6BDE-Fc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4b44423-0325-458f-9ded-0cb165dbec81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[1 / 50] Train: Loss = 0.77748, Accuracy = 77.15%: 100%|██████████| 572/572 [00:11<00:00, 49.85it/s]\n",
            "[1 / 50]   Val: Loss = 0.37732, Accuracy = 89.08%: 100%|██████████| 13/13 [00:00<00:00, 44.08it/s]\n",
            "[2 / 50] Train: Loss = 0.28870, Accuracy = 91.38%: 100%|██████████| 572/572 [00:10<00:00, 53.18it/s]\n",
            "[2 / 50]   Val: Loss = 0.25844, Accuracy = 92.01%: 100%|██████████| 13/13 [00:00<00:00, 48.54it/s]\n",
            "[3 / 50] Train: Loss = 0.21085, Accuracy = 93.42%: 100%|██████████| 572/572 [00:10<00:00, 52.60it/s]\n",
            "[3 / 50]   Val: Loss = 0.21091, Accuracy = 93.37%: 100%|██████████| 13/13 [00:00<00:00, 47.18it/s]\n",
            "[4 / 50] Train: Loss = 0.17423, Accuracy = 94.45%: 100%|██████████| 572/572 [00:10<00:00, 54.10it/s]\n",
            "[4 / 50]   Val: Loss = 0.18529, Accuracy = 94.06%: 100%|██████████| 13/13 [00:00<00:00, 49.30it/s]\n",
            "[5 / 50] Train: Loss = 0.15317, Accuracy = 95.06%: 100%|██████████| 572/572 [00:10<00:00, 53.98it/s]\n",
            "[5 / 50]   Val: Loss = 0.17029, Accuracy = 94.47%: 100%|██████████| 13/13 [00:00<00:00, 47.01it/s]\n",
            "[6 / 50] Train: Loss = 0.13912, Accuracy = 95.44%: 100%|██████████| 572/572 [00:10<00:00, 53.73it/s]\n",
            "[6 / 50]   Val: Loss = 0.15968, Accuracy = 94.77%: 100%|██████████| 13/13 [00:00<00:00, 46.94it/s]\n",
            "[7 / 50] Train: Loss = 0.12925, Accuracy = 95.70%: 100%|██████████| 572/572 [00:10<00:00, 54.00it/s]\n",
            "[7 / 50]   Val: Loss = 0.15374, Accuracy = 94.96%: 100%|██████████| 13/13 [00:00<00:00, 47.66it/s]\n",
            "[8 / 50] Train: Loss = 0.12163, Accuracy = 95.91%: 100%|██████████| 572/572 [00:10<00:00, 54.04it/s]\n",
            "[8 / 50]   Val: Loss = 0.14788, Accuracy = 95.02%: 100%|██████████| 13/13 [00:00<00:00, 49.63it/s]\n",
            "[9 / 50] Train: Loss = 0.11589, Accuracy = 96.08%: 100%|██████████| 572/572 [00:10<00:00, 54.25it/s]\n",
            "[9 / 50]   Val: Loss = 0.14610, Accuracy = 95.13%: 100%|██████████| 13/13 [00:00<00:00, 49.52it/s]\n",
            "[10 / 50] Train: Loss = 0.11118, Accuracy = 96.22%: 100%|██████████| 572/572 [00:10<00:00, 55.61it/s]\n",
            "[10 / 50]   Val: Loss = 0.14179, Accuracy = 95.22%: 100%|██████████| 13/13 [00:00<00:00, 48.20it/s]\n",
            "[11 / 50] Train: Loss = 0.10711, Accuracy = 96.32%: 100%|██████████| 572/572 [00:10<00:00, 55.53it/s]\n",
            "[11 / 50]   Val: Loss = 0.14004, Accuracy = 95.28%: 100%|██████████| 13/13 [00:00<00:00, 49.39it/s]\n",
            "[12 / 50] Train: Loss = 0.10369, Accuracy = 96.42%: 100%|██████████| 572/572 [00:10<00:00, 56.18it/s]\n",
            "[12 / 50]   Val: Loss = 0.13718, Accuracy = 95.32%: 100%|██████████| 13/13 [00:00<00:00, 49.61it/s]\n",
            "[13 / 50] Train: Loss = 0.10091, Accuracy = 96.51%: 100%|██████████| 572/572 [00:10<00:00, 55.47it/s]\n",
            "[13 / 50]   Val: Loss = 0.13807, Accuracy = 95.33%: 100%|██████████| 13/13 [00:00<00:00, 48.92it/s]\n",
            "[14 / 50] Train: Loss = 0.09827, Accuracy = 96.57%: 100%|██████████| 572/572 [00:10<00:00, 55.33it/s]\n",
            "[14 / 50]   Val: Loss = 0.13422, Accuracy = 95.42%: 100%|██████████| 13/13 [00:00<00:00, 47.84it/s]\n",
            "[15 / 50] Train: Loss = 0.09592, Accuracy = 96.64%: 100%|██████████| 572/572 [00:10<00:00, 56.32it/s]\n",
            "[15 / 50]   Val: Loss = 0.13459, Accuracy = 95.31%: 100%|██████████| 13/13 [00:00<00:00, 51.75it/s]\n",
            "[16 / 50] Train: Loss = 0.09373, Accuracy = 96.73%: 100%|██████████| 572/572 [00:10<00:00, 55.07it/s]\n",
            "[16 / 50]   Val: Loss = 0.13462, Accuracy = 95.45%: 100%|██████████| 13/13 [00:00<00:00, 50.40it/s]\n",
            "[17 / 50] Train: Loss = 0.09172, Accuracy = 96.79%: 100%|██████████| 572/572 [00:10<00:00, 56.08it/s]\n",
            "[17 / 50]   Val: Loss = 0.13340, Accuracy = 95.38%: 100%|██████████| 13/13 [00:00<00:00, 54.10it/s]\n",
            "[18 / 50] Train: Loss = 0.09003, Accuracy = 96.86%: 100%|██████████| 572/572 [00:10<00:00, 55.76it/s]\n",
            "[18 / 50]   Val: Loss = 0.13321, Accuracy = 95.37%: 100%|██████████| 13/13 [00:00<00:00, 50.01it/s]\n",
            "[19 / 50] Train: Loss = 0.08839, Accuracy = 96.89%: 100%|██████████| 572/572 [00:10<00:00, 55.76it/s]\n",
            "[19 / 50]   Val: Loss = 0.13323, Accuracy = 95.42%: 100%|██████████| 13/13 [00:00<00:00, 47.28it/s]\n",
            "[20 / 50] Train: Loss = 0.08676, Accuracy = 96.94%: 100%|██████████| 572/572 [00:10<00:00, 55.23it/s]\n",
            "[20 / 50]   Val: Loss = 0.13227, Accuracy = 95.50%: 100%|██████████| 13/13 [00:00<00:00, 47.51it/s]\n",
            "[21 / 50] Train: Loss = 0.08545, Accuracy = 96.98%: 100%|██████████| 572/572 [00:10<00:00, 54.39it/s]\n",
            "[21 / 50]   Val: Loss = 0.13243, Accuracy = 95.40%: 100%|██████████| 13/13 [00:00<00:00, 47.76it/s]\n",
            "[22 / 50] Train: Loss = 0.08395, Accuracy = 97.04%: 100%|██████████| 572/572 [00:10<00:00, 55.45it/s]\n",
            "[22 / 50]   Val: Loss = 0.13497, Accuracy = 95.49%: 100%|██████████| 13/13 [00:00<00:00, 48.79it/s]\n",
            "[23 / 50] Train: Loss = 0.08291, Accuracy = 97.08%: 100%|██████████| 572/572 [00:10<00:00, 54.30it/s]\n",
            "[23 / 50]   Val: Loss = 0.13278, Accuracy = 95.37%: 100%|██████████| 13/13 [00:00<00:00, 48.64it/s]\n",
            "[24 / 50] Train: Loss = 0.08167, Accuracy = 97.09%: 100%|██████████| 572/572 [00:10<00:00, 53.77it/s]\n",
            "[24 / 50]   Val: Loss = 0.13345, Accuracy = 95.45%: 100%|██████████| 13/13 [00:00<00:00, 47.77it/s]\n",
            "[25 / 50] Train: Loss = 0.08064, Accuracy = 97.14%: 100%|██████████| 572/572 [00:10<00:00, 52.72it/s]\n",
            "[25 / 50]   Val: Loss = 0.13289, Accuracy = 95.43%: 100%|██████████| 13/13 [00:00<00:00, 48.61it/s]\n",
            "[26 / 50] Train: Loss = 0.07938, Accuracy = 97.17%: 100%|██████████| 572/572 [00:10<00:00, 52.52it/s]\n",
            "[26 / 50]   Val: Loss = 0.13373, Accuracy = 95.43%: 100%|██████████| 13/13 [00:00<00:00, 49.72it/s]\n",
            "[27 / 50] Train: Loss = 0.07829, Accuracy = 97.23%: 100%|██████████| 572/572 [00:10<00:00, 53.14it/s]\n",
            "[27 / 50]   Val: Loss = 0.13385, Accuracy = 95.40%: 100%|██████████| 13/13 [00:00<00:00, 44.24it/s]\n",
            "[28 / 50] Train: Loss = 0.07758, Accuracy = 97.26%: 100%|██████████| 572/572 [00:10<00:00, 52.22it/s]\n",
            "[28 / 50]   Val: Loss = 0.13439, Accuracy = 95.36%: 100%|██████████| 13/13 [00:00<00:00, 45.30it/s]\n",
            "[29 / 50] Train: Loss = 0.07649, Accuracy = 97.27%: 100%|██████████| 572/572 [00:11<00:00, 51.04it/s]\n",
            "[29 / 50]   Val: Loss = 0.13423, Accuracy = 95.36%: 100%|██████████| 13/13 [00:00<00:00, 47.31it/s]\n",
            "[30 / 50] Train: Loss = 0.07554, Accuracy = 97.31%: 100%|██████████| 572/572 [00:11<00:00, 49.44it/s]\n",
            "[30 / 50]   Val: Loss = 0.13520, Accuracy = 95.42%: 100%|██████████| 13/13 [00:00<00:00, 44.22it/s]\n",
            "[31 / 50] Train: Loss = 0.07485, Accuracy = 97.33%: 100%|██████████| 572/572 [00:11<00:00, 48.50it/s]\n",
            "[31 / 50]   Val: Loss = 0.13551, Accuracy = 95.38%: 100%|██████████| 13/13 [00:00<00:00, 45.67it/s]\n",
            "[32 / 50] Train: Loss = 0.07411, Accuracy = 97.35%: 100%|██████████| 572/572 [00:11<00:00, 49.62it/s]\n",
            "[32 / 50]   Val: Loss = 0.13461, Accuracy = 95.44%: 100%|██████████| 13/13 [00:00<00:00, 46.02it/s]\n",
            "[33 / 50] Train: Loss = 0.07310, Accuracy = 97.37%: 100%|██████████| 572/572 [00:11<00:00, 50.77it/s]\n",
            "[33 / 50]   Val: Loss = 0.13469, Accuracy = 95.37%: 100%|██████████| 13/13 [00:00<00:00, 46.21it/s]\n",
            "[34 / 50] Train: Loss = 0.07259, Accuracy = 97.42%: 100%|██████████| 572/572 [00:11<00:00, 51.20it/s]\n",
            "[34 / 50]   Val: Loss = 0.13641, Accuracy = 95.35%: 100%|██████████| 13/13 [00:00<00:00, 46.66it/s]\n",
            "[35 / 50] Train: Loss = 0.07176, Accuracy = 97.44%: 100%|██████████| 572/572 [00:10<00:00, 52.52it/s]\n",
            "[35 / 50]   Val: Loss = 0.13465, Accuracy = 95.38%: 100%|██████████| 13/13 [00:00<00:00, 45.54it/s]\n",
            "[36 / 50] Train: Loss = 0.07075, Accuracy = 97.49%: 100%|██████████| 572/572 [00:10<00:00, 53.39it/s]\n",
            "[36 / 50]   Val: Loss = 0.13600, Accuracy = 95.44%: 100%|██████████| 13/13 [00:00<00:00, 46.83it/s]\n",
            "[37 / 50] Train: Loss = 0.07020, Accuracy = 97.48%: 100%|██████████| 572/572 [00:10<00:00, 54.01it/s]\n",
            "[37 / 50]   Val: Loss = 0.13790, Accuracy = 95.34%: 100%|██████████| 13/13 [00:00<00:00, 49.54it/s]\n",
            "[38 / 50] Train: Loss = 0.06957, Accuracy = 97.52%: 100%|██████████| 572/572 [00:10<00:00, 52.79it/s]\n",
            "[38 / 50]   Val: Loss = 0.13612, Accuracy = 95.37%: 100%|██████████| 13/13 [00:00<00:00, 45.46it/s]\n",
            "[39 / 50] Train: Loss = 0.06890, Accuracy = 97.56%: 100%|██████████| 572/572 [00:10<00:00, 53.71it/s]\n",
            "[39 / 50]   Val: Loss = 0.13803, Accuracy = 95.46%: 100%|██████████| 13/13 [00:00<00:00, 47.91it/s]\n",
            "[40 / 50] Train: Loss = 0.06815, Accuracy = 97.57%: 100%|██████████| 572/572 [00:10<00:00, 53.99it/s]\n",
            "[40 / 50]   Val: Loss = 0.13920, Accuracy = 95.22%: 100%|██████████| 13/13 [00:00<00:00, 48.17it/s]\n",
            "[41 / 50] Train: Loss = 0.06752, Accuracy = 97.59%: 100%|██████████| 572/572 [00:10<00:00, 54.95it/s]\n",
            "[41 / 50]   Val: Loss = 0.13913, Accuracy = 95.38%: 100%|██████████| 13/13 [00:00<00:00, 48.39it/s]\n",
            "[42 / 50] Train: Loss = 0.06719, Accuracy = 97.59%: 100%|██████████| 572/572 [00:10<00:00, 54.36it/s]\n",
            "[42 / 50]   Val: Loss = 0.13982, Accuracy = 95.26%: 100%|██████████| 13/13 [00:00<00:00, 47.59it/s]\n",
            "[43 / 50] Train: Loss = 0.06631, Accuracy = 97.64%: 100%|██████████| 572/572 [00:10<00:00, 52.08it/s]\n",
            "[43 / 50]   Val: Loss = 0.14154, Accuracy = 95.29%: 100%|██████████| 13/13 [00:00<00:00, 47.71it/s]\n",
            "[44 / 50] Train: Loss = 0.06586, Accuracy = 97.66%: 100%|██████████| 572/572 [00:10<00:00, 53.11it/s]\n",
            "[44 / 50]   Val: Loss = 0.14157, Accuracy = 95.27%: 100%|██████████| 13/13 [00:00<00:00, 46.61it/s]\n",
            "[45 / 50] Train: Loss = 0.06518, Accuracy = 97.68%: 100%|██████████| 572/572 [00:10<00:00, 52.77it/s]\n",
            "[45 / 50]   Val: Loss = 0.14251, Accuracy = 95.21%: 100%|██████████| 13/13 [00:00<00:00, 47.76it/s]\n",
            "[46 / 50] Train: Loss = 0.06470, Accuracy = 97.70%: 100%|██████████| 572/572 [00:10<00:00, 53.35it/s]\n",
            "[46 / 50]   Val: Loss = 0.14178, Accuracy = 95.32%: 100%|██████████| 13/13 [00:00<00:00, 46.86it/s]\n",
            "[47 / 50] Train: Loss = 0.06409, Accuracy = 97.71%: 100%|██████████| 572/572 [00:10<00:00, 52.32it/s]\n",
            "[47 / 50]   Val: Loss = 0.14359, Accuracy = 95.26%: 100%|██████████| 13/13 [00:00<00:00, 45.86it/s]\n",
            "[48 / 50] Train: Loss = 0.06353, Accuracy = 97.73%: 100%|██████████| 572/572 [00:10<00:00, 52.94it/s]\n",
            "[48 / 50]   Val: Loss = 0.14532, Accuracy = 95.25%: 100%|██████████| 13/13 [00:00<00:00, 45.63it/s]\n",
            "[49 / 50] Train: Loss = 0.06309, Accuracy = 97.73%: 100%|██████████| 572/572 [00:10<00:00, 52.49it/s]\n",
            "[49 / 50]   Val: Loss = 0.14623, Accuracy = 95.22%: 100%|██████████| 13/13 [00:00<00:00, 41.89it/s]\n",
            "[50 / 50] Train: Loss = 0.06268, Accuracy = 97.76%: 100%|██████████| 572/572 [00:11<00:00, 51.99it/s]\n",
            "[50 / 50]   Val: Loss = 0.14611, Accuracy = 95.34%: 100%|██████████| 13/13 [00:00<00:00, 43.91it/s]\n"
          ]
        }
      ],
      "source": [
        "model = LSTMTaggerWithPretrainedEmbs(\n",
        "    embeddings=embeddings,\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ne_8f24h8kg"
      },
      "source": [
        "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
        "\n",
        "Добейтесь качества лучше прошлых моделей."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "HPUuAPGhEGVR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "117d05d4-c44f-4e30-f806-fc61cfaa09cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9536129500910596"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "correct_count = 0\n",
        "sum_count = 0\n",
        "\n",
        "for X_batch, y_batch in iterate_batches((X_test, y_test), 64):\n",
        "    X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "    logits = model(X_batch)\n",
        "\n",
        "    preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    mask = (y_batch != 0).float()\n",
        "    cur_correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
        "    cur_sum_count = mask.sum().item()\n",
        "                \n",
        "    correct_count += cur_correct_count\n",
        "    sum_count += cur_sum_count\n",
        "\n",
        "correct_count / sum_count"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "RNNs.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}